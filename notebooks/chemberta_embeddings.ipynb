{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "236ba126",
   "metadata": {},
   "source": [
    "Here, I try to obtain embeddings of the given smiles using chemberta so that later, we can use different types of simple machine learning models to predict their properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15669aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "77a36013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../models/ChemBERTa-77M-MLM. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "# model = SentenceTransformer(\"Derify/ChemMRL-alpha\")\n",
    "model = SentenceTransformer('../models/ChemBERTa-77M-MLM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d2bc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7973, 7)\n",
      "Test shape: (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df_train = pd.read_csv('../data/raw/neurips-open-polymer-prediction-2025/train.csv')\n",
    "df_test = pd.read_csv('../data/raw/neurips-open-polymer-prediction-2025/test.csv')\n",
    "print(f'Train shape: {df_train.shape}')\n",
    "print(f'Test shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e0abc",
   "metadata": {},
   "source": [
    "### Simple model, Nadaraya-Watson regression for each property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02fcb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the subsets of the df, where each subset has all the information for one property\n",
    "data_tg = df_train.loc[~df_train['Tg'].isna(), ['id', 'SMILES', 'Tg']]\n",
    "data_ffv = df_train.loc[~df_train['FFV'].isna(), ['id', 'SMILES', 'FFV']]\n",
    "data_tc = df_train.loc[~df_train['Tc'].isna(), ['id', 'SMILES', 'Tc']]\n",
    "data_density = df_train.loc[~df_train['Density'].isna(), ['id', 'SMILES', 'Density']]\n",
    "data_rg = df_train.loc[~df_train['Rg'].isna(), ['id', 'SMILES', 'Rg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "df5f1c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(df_train['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f9ba8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class holding all information of the regressor we are creating in one place\n",
    "class NWRegressor():\n",
    "    def __init__(self, df_train, model, kernel_bandwidth):\n",
    "        # store the model as an attribute\n",
    "        self.model = model\n",
    "        # store as an attribute, the bandwidth of the kernel\n",
    "        self.bandwidth = kernel_bandwidth\n",
    "        # the df_train is also an integral part of the object, so should be stored as an attribute\n",
    "        self.df_train = df_train\n",
    "        # store all the data-subset attributes\n",
    "        self.data_tg = df_train.loc[~df_train['Tg'].isna(), ['id', 'SMILES', 'Tg']]\n",
    "        self.data_ffv = df_train.loc[~df_train['FFV'].isna(), ['id', 'SMILES', 'FFV']]\n",
    "        self.data_tc = df_train.loc[~df_train['Tc'].isna(), ['id', 'SMILES', 'Tc']]\n",
    "        self.data_density = df_train.loc[~df_train['Density'].isna(), ['id', 'SMILES', 'Density']]\n",
    "        self.data_rg = df_train.loc[~df_train['Rg'].isna(), ['id', 'SMILES', 'Rg']]\n",
    "\n",
    "    # separate method to initialize and assign embeddings because sometimes we may have them already computed and just want to assign so there is no need to do this in __init__\n",
    "    def store_embeddings(self):\n",
    "        # store the embeddings of all the smiles in df_train\n",
    "        self.embeddings = self.model.encode(self.df_train['SMILES']) # shape: (nrow(df_train), 1024) = (7973, 1024)\n",
    "\n",
    "    def kernel_fn(self, x):\n",
    "        numerator = -(x * x)\n",
    "        denominator = 2 * self.bandwidth * self.bandwidth\n",
    "        return np.exp(numerator/denominator)\n",
    "    \n",
    "    def distance(self, arr1, arr2):\n",
    "        # Compute pairwise distances (Euclidean by default)\n",
    "        arr1_exp = arr1[:, np.newaxis, :]  # (x, 1, 120)\n",
    "        arr2_exp = arr2[np.newaxis, :, :]  # (1, y, 120)\n",
    "        sq_diff = (arr1_exp - arr2_exp) ** 2  # (x, y, 120)\n",
    "        sq_distances = np.sum(sq_diff, axis=2)  # (x, y)\n",
    "        distances = np.sqrt(sq_distances)  # (x, y)\n",
    "        return distances\n",
    "\n",
    "    # data_property is the dataframe corresponding to a certain property for which predictions are being made\n",
    "    def predict_property(self, test_embeddings, data_property):\n",
    "        # find out the distances between test_embeddings and self.embeddings' subset of data_property\n",
    "        dists = self.distance(self.embeddings[data_property.index], test_embeddings) # shape: (nrow(data_property), test_embeddings)\n",
    "        weights = self.kernel_fn(dists) # pass the mere distances to kernel to get weights\n",
    "        # now, we scale the kernel-passed weights so that for each test_embedding, the weights' sum is 1\n",
    "        weights_scaled = weights/weights.sum(axis = 0)\n",
    "        # now we just tranpose as test_embeddings being as rownames are more intuitive\n",
    "        weights_scaled = weights_scaled.T # shape: (test_embeddings,nrow(data_property))\n",
    "        # now, just perform dot product with tg values vector to get tg value for each test_embedding\n",
    "        train_tg_values = data_property.iloc[:, -1].values\n",
    "        output_property_values = np.matmul(weights_scaled, train_tg_values)\n",
    "        return output_property_values # shape: (test_embeddings,)\n",
    "\n",
    "        # HERE... go through the details of the nadaraya-watson implementation. Where scaling, softmax etc. Then, finally\n",
    "        # use the scaled weights to compute the avg value of tg.\n",
    "    \n",
    "    # predict on a list of smiles\n",
    "    def predict(self, smiles):\n",
    "        # create empty numpy arrays on which we can append as we get more and more predictions\n",
    "        tg = np.empty(0)\n",
    "        ffv = np.empty(0)\n",
    "        tc = np.empty(0)\n",
    "        density = np.empty(0)\n",
    "        rg = np.empty(0)\n",
    "        # loop is for being able to treat big list of smiles in parts\n",
    "        for i in range(0, len(smiles), 1):\n",
    "            smiles_subset = smiles[i:i+1000]\n",
    "            test_embeddings = self.model.encode(smiles_subset)\n",
    "            # call the predict method for each property and return the values\n",
    "            tg = np.concatenate([tg, self.predict_property(test_embeddings, self.data_tg)])\n",
    "            ffv = np.concatenate([ffv, self.predict_property(test_embeddings, self.data_ffv)])\n",
    "            tc = np.concatenate([tc, self.predict_property(test_embeddings, self.data_tc)])\n",
    "            density = np.concatenate([density, self.predict_property(test_embeddings, self.data_density)])\n",
    "            rg = np.concatenate([rg, self.predict_property(test_embeddings, self.data_rg)])\n",
    "        \n",
    "        return {'Tg': tg, 'FFV': ffv, 'Tc': tc, 'Density': density, 'Rg': rg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "92209eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding good bandwidth using validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train_subset1, df_train_subset2 = train_test_split(df_train, test_size=0.05, train_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e796f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting indices is important as the NWRegressor class assumes the df to have index in proper order\n",
    "df_train_subset1 = df_train_subset1.reset_index(drop = True)\n",
    "df_train_subset2 = df_train_subset2.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4dfd50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_df_train_subset1 = model.encode(list(df_train_subset1['SMILES']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "3e5fe153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "      <th>FFV</th>\n",
       "      <th>Tc</th>\n",
       "      <th>Density</th>\n",
       "      <th>Rg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87817</td>\n",
       "      <td>*CC(*)c1ccccc1C(=O)OCCCCCC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374645</td>\n",
       "      <td>0.205667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>106919</td>\n",
       "      <td>*Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.370410</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>388772</td>\n",
       "      <td>*Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.378860</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>519416</td>\n",
       "      <td>*Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.387324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539187</td>\n",
       "      <td>*Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.355470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7968</th>\n",
       "      <td>2146592435</td>\n",
       "      <td>*Oc1cc(CCCCCCCC)cc(OC(=O)c2cccc(C(*)=O)c2)c1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.367498</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7969</th>\n",
       "      <td>2146810552</td>\n",
       "      <td>*C(=O)OCCN(CCOC(=O)c1ccc2c(c1)C(=O)N(c1cccc(N3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.353280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7970</th>\n",
       "      <td>2147191531</td>\n",
       "      <td>*c1cc(C(=O)NCCCCCCCC)cc(N2C(=O)c3ccc(-c4ccc5c(...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.369411</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>2147435020</td>\n",
       "      <td>*C=C(*)c1ccccc1C</td>\n",
       "      <td>261.662355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>2147438299</td>\n",
       "      <td>*c1ccc(OCCCCCCCCCCCOC(=O)CCCCC(=O)OCCCCCCCCCCC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.374049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7973 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                             SMILES  \\\n",
       "0          87817                         *CC(*)c1ccccc1C(=O)OCCCCCC   \n",
       "1         106919  *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5...   \n",
       "2         388772  *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(...   \n",
       "3         519416  *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)...   \n",
       "4         539187  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N...   \n",
       "...          ...                                                ...   \n",
       "7968  2146592435       *Oc1cc(CCCCCCCC)cc(OC(=O)c2cccc(C(*)=O)c2)c1   \n",
       "7969  2146810552  *C(=O)OCCN(CCOC(=O)c1ccc2c(c1)C(=O)N(c1cccc(N3...   \n",
       "7970  2147191531  *c1cc(C(=O)NCCCCCCCC)cc(N2C(=O)c3ccc(-c4ccc5c(...   \n",
       "7971  2147435020                                   *C=C(*)c1ccccc1C   \n",
       "7972  2147438299  *c1ccc(OCCCCCCCCCCCOC(=O)CCCCC(=O)OCCCCCCCCCCC...   \n",
       "\n",
       "              Tg       FFV        Tc  Density  Rg  \n",
       "0            NaN  0.374645  0.205667      NaN NaN  \n",
       "1            NaN  0.370410       NaN      NaN NaN  \n",
       "2            NaN  0.378860       NaN      NaN NaN  \n",
       "3            NaN  0.387324       NaN      NaN NaN  \n",
       "4            NaN  0.355470       NaN      NaN NaN  \n",
       "...          ...       ...       ...      ...  ..  \n",
       "7968         NaN  0.367498       NaN      NaN NaN  \n",
       "7969         NaN  0.353280       NaN      NaN NaN  \n",
       "7970         NaN  0.369411       NaN      NaN NaN  \n",
       "7971  261.662355       NaN       NaN      NaN NaN  \n",
       "7972         NaN  0.374049       NaN      NaN NaN  \n",
       "\n",
       "[7973 rows x 7 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "668fb875",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 5\n",
    "regressor = NWRegressor(df_train_subset1, model, bw)\n",
    "regressor.embeddings = embeddings_df_train_subset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "12a633db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tg': array([96.9021827 , 96.86489382, 97.24017122, 97.08235879, 97.12738053,\n",
       "        97.30993194, 96.41359025, 96.93743498, 97.02886353, 97.30738959,\n",
       "        97.34384608, 97.30927689, 96.51444131, 96.47728191, 97.32933266,\n",
       "        97.40552919, 97.42008   , 97.28600103, 97.31883791, 97.11382146,\n",
       "        97.10139481, 97.17634599, 97.1814653 , 96.40158375, 97.02794687,\n",
       "        97.11619299, 96.44739131, 96.73368298, 96.78446292, 96.80254243,\n",
       "        97.24679163, 97.03527212, 97.28839675, 97.28476433, 96.86038435,\n",
       "        96.85009155, 97.29550642, 97.34383014, 96.50819611, 96.51558432,\n",
       "        96.77138167, 97.31019825, 97.16751858, 97.02672698, 97.16551505,\n",
       "        96.70325237, 97.36286234, 97.41203767, 97.02902145, 96.50791265,\n",
       "        96.87759389, 97.38628484, 97.18247764, 97.02217287, 96.9863213 ,\n",
       "        97.14216936, 97.36011206, 97.28390467, 96.43026674, 96.73843746,\n",
       "        96.82060294, 96.87652483, 96.57402842, 96.72118769, 96.99451865,\n",
       "        97.3303303 , 97.46580452, 97.09954597, 96.77758012, 96.56243105,\n",
       "        97.3801815 , 97.41895514, 97.18346345, 97.35655855, 97.45588502,\n",
       "        96.48025608, 97.29242528, 96.53993197, 96.5465967 , 97.24788287,\n",
       "        96.95202873, 96.8042277 , 96.67072282, 97.17586859, 96.63648957,\n",
       "        97.30304071, 97.25499275, 96.97792172, 96.97357773, 97.0756879 ,\n",
       "        97.03599448, 96.92364944, 96.74464652, 97.00276598, 97.03016624,\n",
       "        97.09909124, 97.11671074, 97.18934559, 97.01478046, 97.10117356,\n",
       "        97.04441494, 97.41306055, 97.34931342, 97.19313196, 97.44461747,\n",
       "        96.7850257 , 96.52333427, 97.19354637, 96.73794019, 97.44247144,\n",
       "        96.80122316, 96.86477724, 96.59943731, 97.34463091, 97.12665374,\n",
       "        97.37892554, 97.0087847 , 96.94829485, 96.87383866, 96.74325015,\n",
       "        97.21156324, 96.6470229 , 96.84607139, 97.14694665, 97.14390677,\n",
       "        97.30949391, 96.88149751, 96.67669353, 97.35066933, 97.24149309,\n",
       "        96.742874  , 96.69548958, 96.862924  , 96.53720823, 97.4133527 ,\n",
       "        96.55353158, 97.35843509, 97.34938447, 97.37339969, 96.8895252 ,\n",
       "        97.33021239, 97.09834626, 97.26772534, 97.25677309, 96.69982332,\n",
       "        97.36018322, 96.76034678, 97.22486129, 97.24511029, 97.3389858 ,\n",
       "        96.78854179, 97.4828356 , 97.31444124, 97.26853731, 97.01971957,\n",
       "        97.29956322, 97.39363814, 96.99819987, 97.21090377, 97.16542571,\n",
       "        97.36412048, 96.75654752, 96.93483931, 96.73191717, 97.4033512 ,\n",
       "        96.49061121, 96.52712843, 97.21557781, 96.80613461, 97.42570066,\n",
       "        97.08996587, 97.26255959, 97.41347328, 97.40439215, 96.93205305,\n",
       "        97.11089854, 96.80857381, 97.35034727, 96.99072684, 97.10180941,\n",
       "        97.30456415, 97.13193464, 96.97777033, 96.80742469, 97.41399863,\n",
       "        97.03772996, 97.04434264, 97.35398192, 96.56610813, 96.69082038,\n",
       "        97.10611494, 96.55248572, 96.93591505, 97.22290378, 97.41089828,\n",
       "        96.73222851, 96.83492587, 97.06003862, 97.24857156, 97.14088063,\n",
       "        96.64480121, 96.78332751, 96.79141153, 96.7364811 , 96.88516114,\n",
       "        97.00614358, 97.32147529, 97.27074339, 97.35797932, 97.24454271,\n",
       "        97.02367067, 96.81834918, 96.81397776, 97.14918375, 96.67282319,\n",
       "        96.53589104, 96.73530485, 97.29472151, 97.28810282, 97.24590066,\n",
       "        97.07375089, 97.24596683, 97.27835653, 97.14325466, 97.30100186,\n",
       "        96.78109582, 97.16161395, 97.36990992, 97.35237328, 96.45732795,\n",
       "        97.16688314, 97.00045519, 96.82125553, 97.27921764, 96.57594338,\n",
       "        96.48597511, 97.26095094, 97.29896266, 96.57934994, 97.2465601 ,\n",
       "        97.14253552, 96.70562868, 96.89417869, 97.23345199, 96.48734352,\n",
       "        97.38143927, 97.39081356, 96.90913682, 96.96403833, 96.82113917,\n",
       "        96.87207409, 97.32118085, 97.17832038, 97.37081011, 96.54024671,\n",
       "        97.00322983, 96.74180964, 97.31232913, 97.23086543, 96.90340772,\n",
       "        97.25774998, 97.00746073, 97.23882272, 96.98823641, 96.8250839 ,\n",
       "        97.46404698, 96.91003338, 97.34991546, 97.44288376, 97.40097894,\n",
       "        97.2611284 , 96.79161699, 97.32000874, 96.66968285, 96.48403664,\n",
       "        97.2829693 , 96.82807023, 97.08233643, 96.59952473, 97.354741  ,\n",
       "        97.3096884 , 97.3986243 , 96.85874966, 96.55257592, 96.73503598,\n",
       "        96.57211257, 96.91260807, 97.27518523, 97.33857849, 97.38264773,\n",
       "        96.45326241, 97.33454568, 96.84197667, 96.77557776, 97.17126056,\n",
       "        97.43158219, 97.34450566, 97.11753917, 96.57083548, 97.0107915 ,\n",
       "        97.13014426, 97.09058981, 97.38817381, 97.40737006, 97.32674853,\n",
       "        96.83176213, 96.55512765, 97.09039872, 97.41409063, 96.55191187,\n",
       "        96.79923564, 96.95436218, 97.22456677, 97.12397453, 97.27312842,\n",
       "        97.4027647 , 96.56964105, 96.9641734 , 96.82791812, 97.35363912,\n",
       "        97.3442738 , 97.32640495, 97.24399307, 96.92405047, 96.91847208,\n",
       "        97.17777687, 96.75506104, 96.9467642 , 96.4677978 , 96.51941646,\n",
       "        97.31794203, 97.28962824, 97.22048085, 97.19985582, 97.02979682,\n",
       "        97.11619179, 97.15496258, 97.23309071, 96.69370178, 97.2087964 ,\n",
       "        97.17321876, 96.83696446, 96.53708374, 96.91934136, 97.26494488,\n",
       "        97.04418411, 96.71151018, 97.29443295, 96.75753475, 97.38232856,\n",
       "        97.37655507, 96.47314859, 96.65045041, 96.43521582, 96.87753723,\n",
       "        97.11225053, 97.2328084 , 97.27930968, 97.2202387 , 96.60155822,\n",
       "        96.69459578, 96.78731642, 96.91897814, 96.67703073, 96.83201795,\n",
       "        96.44377432, 96.7775354 , 96.46590838, 96.95613911, 96.50173061,\n",
       "        97.22928933, 97.37670861, 97.34914061, 97.28899518, 97.32421007,\n",
       "        96.94201877, 97.24165373, 96.93845657, 96.90450935, 97.20905396,\n",
       "        97.00663467, 96.44806608, 96.86012618, 97.33863381, 97.16864362,\n",
       "        96.90465558, 97.39507291, 96.72796012, 96.96464645, 96.62573489,\n",
       "        97.40287383, 96.48895131, 96.95255061, 96.42783056, 96.71426938,\n",
       "        97.31911012, 96.76330654, 96.69189471, 96.91703583]),\n",
       " 'FFV': array([0.36717889, 0.36719656, 0.36720283, 0.36716755, 0.36724679,\n",
       "        0.36723298, 0.36712762, 0.36717725, 0.36721847, 0.36721773,\n",
       "        0.36725495, 0.36721372, 0.36718534, 0.36716853, 0.36723673,\n",
       "        0.36723714, 0.3672351 , 0.36722864, 0.36721891, 0.36720157,\n",
       "        0.36721266, 0.36720691, 0.36721179, 0.36712966, 0.36719302,\n",
       "        0.3671841 , 0.36713133, 0.36717746, 0.36718766, 0.36714095,\n",
       "        0.36721143, 0.36719225, 0.36721218, 0.36722887, 0.36717151,\n",
       "        0.3671828 , 0.36721862, 0.36722624, 0.36713731, 0.36714008,\n",
       "        0.36714141, 0.36723563, 0.36722899, 0.36719231, 0.36722358,\n",
       "        0.36716859, 0.36723213, 0.36723848, 0.36717299, 0.36715352,\n",
       "        0.36717157, 0.36722387, 0.36722641, 0.36722574, 0.36717145,\n",
       "        0.36722215, 0.3672091 , 0.36722078, 0.3671351 , 0.36714654,\n",
       "        0.36715576, 0.36719122, 0.36714756, 0.36715063, 0.36717851,\n",
       "        0.36721777, 0.36726915, 0.36719676, 0.36714433, 0.36714238,\n",
       "        0.36723501, 0.36725383, 0.36722254, 0.36724083, 0.36726397,\n",
       "        0.36713559, 0.36720843, 0.36714688, 0.3671507 , 0.36722855,\n",
       "        0.36719405, 0.3671704 , 0.36715991, 0.36721459, 0.36716977,\n",
       "        0.36719882, 0.36722525, 0.36721094, 0.36715399, 0.36721219,\n",
       "        0.36720383, 0.36717278, 0.36714463, 0.36719516, 0.36722731,\n",
       "        0.36718556, 0.36720471, 0.36720046, 0.36721075, 0.36721343,\n",
       "        0.36719595, 0.36725633, 0.36722769, 0.36720106, 0.36726036,\n",
       "        0.36718217, 0.36719243, 0.36720062, 0.36717057, 0.36724499,\n",
       "        0.36718864, 0.36716264, 0.36713699, 0.36722906, 0.36715893,\n",
       "        0.36724331, 0.36719756, 0.3671913 , 0.36715496, 0.36714308,\n",
       "        0.36720314, 0.36716113, 0.36719985, 0.36717968, 0.36720935,\n",
       "        0.36721605, 0.36718843, 0.36715466, 0.36724205, 0.36718168,\n",
       "        0.3671469 , 0.36714954, 0.36716292, 0.36717079, 0.3672488 ,\n",
       "        0.36716482, 0.36723625, 0.36722017, 0.36723008, 0.36718382,\n",
       "        0.36723876, 0.36724512, 0.3672156 , 0.36720029, 0.36716315,\n",
       "        0.3672483 , 0.36716182, 0.36721811, 0.36720521, 0.36726532,\n",
       "        0.36718004, 0.36725732, 0.3672385 , 0.36722839, 0.36721462,\n",
       "        0.36721894, 0.36725133, 0.3671981 , 0.36720125, 0.36722748,\n",
       "        0.36727043, 0.36714716, 0.36718201, 0.36716682, 0.36723022,\n",
       "        0.36712145, 0.36714125, 0.36719378, 0.36715098, 0.36724072,\n",
       "        0.36719688, 0.36720169, 0.36725776, 0.36725238, 0.36720813,\n",
       "        0.3672114 , 0.36716719, 0.36722121, 0.36717302, 0.36724087,\n",
       "        0.36722041, 0.3672091 , 0.36719286, 0.36717786, 0.36723867,\n",
       "        0.36716462, 0.36719619, 0.3672485 , 0.36714464, 0.36721011,\n",
       "        0.36722172, 0.36713306, 0.36718672, 0.36717668, 0.36725467,\n",
       "        0.36716876, 0.36717903, 0.36717711, 0.36723011, 0.36720225,\n",
       "        0.36715812, 0.36714073, 0.36717976, 0.3671626 , 0.36718936,\n",
       "        0.36717512, 0.3672329 , 0.36723372, 0.36723014, 0.36721281,\n",
       "        0.36723766, 0.36717744, 0.36717723, 0.36720799, 0.36715574,\n",
       "        0.36714969, 0.36715929, 0.36722556, 0.3672149 , 0.36722534,\n",
       "        0.36721683, 0.36719559, 0.36720643, 0.36724848, 0.36721745,\n",
       "        0.3671807 , 0.36720079, 0.36723189, 0.36723186, 0.36713686,\n",
       "        0.3672204 , 0.36719148, 0.36717261, 0.36721612, 0.36716018,\n",
       "        0.36713313, 0.36723584, 0.36722822, 0.36713845, 0.36721767,\n",
       "        0.36721032, 0.36715621, 0.36716649, 0.36720201, 0.36712114,\n",
       "        0.36723125, 0.36723775, 0.36719133, 0.36719715, 0.36715953,\n",
       "        0.36719135, 0.36722465, 0.36722388, 0.36721289, 0.36714694,\n",
       "        0.36722326, 0.36717686, 0.36722726, 0.3672046 , 0.36719283,\n",
       "        0.36724936, 0.36718961, 0.36722467, 0.36715477, 0.36713681,\n",
       "        0.36726839, 0.36720504, 0.36720857, 0.36724285, 0.36724076,\n",
       "        0.36720828, 0.36714545, 0.36721546, 0.36713307, 0.3671246 ,\n",
       "        0.36720915, 0.36716453, 0.3672349 , 0.3671379 , 0.36725263,\n",
       "        0.36723272, 0.36724892, 0.36716782, 0.36719314, 0.36714302,\n",
       "        0.36716604, 0.36716618, 0.36723337, 0.36722419, 0.3672264 ,\n",
       "        0.36713506, 0.36725385, 0.3671685 , 0.36717999, 0.36717084,\n",
       "        0.36727219, 0.36724264, 0.36718945, 0.36712922, 0.36719839,\n",
       "        0.36717851, 0.36721155, 0.3672486 , 0.36725243, 0.36723817,\n",
       "        0.3671791 , 0.36714732, 0.36716979, 0.36724651, 0.36714694,\n",
       "        0.367174  , 0.36723208, 0.36722631, 0.36720381, 0.36722923,\n",
       "        0.36724002, 0.36716856, 0.36717271, 0.36721094, 0.36722286,\n",
       "        0.36724692, 0.36723488, 0.36722288, 0.36719362, 0.36716465,\n",
       "        0.36723815, 0.3671574 , 0.36717535, 0.36713329, 0.36713082,\n",
       "        0.36721797, 0.36722268, 0.3672137 , 0.36722105, 0.36722025,\n",
       "        0.36720253, 0.3671923 , 0.36720085, 0.36716783, 0.36720333,\n",
       "        0.36718099, 0.36718551, 0.36717485, 0.36717746, 0.36721883,\n",
       "        0.3671882 , 0.36715342, 0.36721465, 0.36716221, 0.36724134,\n",
       "        0.367231  , 0.36714118, 0.36714787, 0.36712134, 0.36719345,\n",
       "        0.36720575, 0.36718212, 0.367222  , 0.36722631, 0.36717187,\n",
       "        0.36716659, 0.36714445, 0.36716907, 0.3671887 , 0.36714452,\n",
       "        0.36712758, 0.36718967, 0.36713786, 0.3672132 , 0.36714433,\n",
       "        0.36725076, 0.3672266 , 0.36723071, 0.36720085, 0.36724058,\n",
       "        0.36719942, 0.36724261, 0.36716805, 0.36720618, 0.36722578,\n",
       "        0.36724346, 0.36712333, 0.36715782, 0.36722256, 0.36723953,\n",
       "        0.36718915, 0.36724815, 0.36715612, 0.36718445, 0.36713863,\n",
       "        0.36723973, 0.36714157, 0.36716615, 0.3671465 , 0.36713417,\n",
       "        0.36721528, 0.36716299, 0.3671685 , 0.36717319]),\n",
       " 'Tc': array([0.25649822, 0.25642574, 0.25673353, 0.25650951, 0.25675306,\n",
       "        0.25668348, 0.25696262, 0.25649775, 0.25675537, 0.25672212,\n",
       "        0.25677301, 0.25657912, 0.25691914, 0.25675934, 0.25664149,\n",
       "        0.2566508 , 0.25664309, 0.2566831 , 0.25660416, 0.25670686,\n",
       "        0.2567608 , 0.25665251, 0.25674952, 0.25699283, 0.25674246,\n",
       "        0.25659112, 0.25711004, 0.25663192, 0.25646731, 0.25667632,\n",
       "        0.25659034, 0.25662885, 0.25657189, 0.25657913, 0.25645544,\n",
       "        0.25665178, 0.25669539, 0.25671961, 0.25648556, 0.25722547,\n",
       "        0.25667894, 0.25674701, 0.25659709, 0.2567285 , 0.2567751 ,\n",
       "        0.25665595, 0.25665523, 0.25663079, 0.25646924, 0.25683962,\n",
       "        0.25652733, 0.25661885, 0.2566755 , 0.25655072, 0.2564993 ,\n",
       "        0.25674498, 0.25665889, 0.25655774, 0.25683264, 0.25679715,\n",
       "        0.25661476, 0.25651185, 0.25639098, 0.25672707, 0.25672255,\n",
       "        0.25664487, 0.25666951, 0.25669609, 0.25664634, 0.25659873,\n",
       "        0.25670163, 0.25662549, 0.25664152, 0.25658207, 0.25671625,\n",
       "        0.25700286, 0.25668981, 0.25651831, 0.2572286 , 0.25662664,\n",
       "        0.25661483, 0.25644817, 0.25687356, 0.25675022, 0.25671335,\n",
       "        0.25669903, 0.25660494, 0.2567309 , 0.25668419, 0.25672668,\n",
       "        0.25675043, 0.25651976, 0.25676959, 0.25670259, 0.25656591,\n",
       "        0.25665576, 0.25664098, 0.25677549, 0.25660771, 0.25671561,\n",
       "        0.25645218, 0.25666202, 0.25668968, 0.25663571, 0.25667394,\n",
       "        0.25689671, 0.25698236, 0.25653152, 0.25651751, 0.25662234,\n",
       "        0.25658707, 0.25663367, 0.2569106 , 0.25674748, 0.25665082,\n",
       "        0.25662187, 0.25663632, 0.25660419, 0.25675912, 0.25674112,\n",
       "        0.25651182, 0.25650083, 0.25652626, 0.2566126 , 0.25659127,\n",
       "        0.25659019, 0.2566427 , 0.25683867, 0.25663518, 0.25660595,\n",
       "        0.256787  , 0.25658638, 0.25654437, 0.2568828 , 0.25660486,\n",
       "        0.25676261, 0.25671204, 0.2566    , 0.25662128, 0.25653775,\n",
       "        0.25662127, 0.25680734, 0.25666392, 0.25662315, 0.25648152,\n",
       "        0.25667201, 0.25664106, 0.25669979, 0.25658743, 0.25673355,\n",
       "        0.2567544 , 0.25668354, 0.25673237, 0.25673369, 0.25679221,\n",
       "        0.25671947, 0.25669267, 0.25676097, 0.25659255, 0.25651258,\n",
       "        0.25673015, 0.25679366, 0.25665438, 0.25650522, 0.25670237,\n",
       "        0.256973  , 0.25642685, 0.25656124, 0.25657786, 0.25666475,\n",
       "        0.25654624, 0.25662252, 0.25673543, 0.25668189, 0.25670805,\n",
       "        0.2566339 , 0.25686442, 0.25668167, 0.25665867, 0.25681344,\n",
       "        0.25670705, 0.25655294, 0.25655771, 0.25637525, 0.25663446,\n",
       "        0.25673083, 0.25668993, 0.25663458, 0.25680679, 0.25698495,\n",
       "        0.25658226, 0.25686845, 0.25661846, 0.25658945, 0.25670892,\n",
       "        0.25661673, 0.25663184, 0.25667009, 0.25663146, 0.25666352,\n",
       "        0.25705785, 0.25676915, 0.25693546, 0.256548  , 0.25676822,\n",
       "        0.25663536, 0.25651696, 0.2566076 , 0.25670266, 0.25662052,\n",
       "        0.25685941, 0.25643858, 0.25662356, 0.25649335, 0.25658797,\n",
       "        0.25724497, 0.25656933, 0.25668833, 0.25668504, 0.25657364,\n",
       "        0.25672571, 0.2566107 , 0.25658346, 0.25678809, 0.25665811,\n",
       "        0.25681709, 0.25664465, 0.25666125, 0.2567512 , 0.25674196,\n",
       "        0.25676815, 0.25674466, 0.25658397, 0.25660991, 0.25654832,\n",
       "        0.25716195, 0.25660246, 0.25659469, 0.25661055, 0.25667611,\n",
       "        0.25663042, 0.25692287, 0.25676001, 0.25666298, 0.25676916,\n",
       "        0.25660796, 0.25671103, 0.25678976, 0.25671867, 0.25656396,\n",
       "        0.25671937, 0.25663243, 0.25668109, 0.25669291, 0.25666227,\n",
       "        0.25657606, 0.25657895, 0.25659569, 0.25671717, 0.25676162,\n",
       "        0.25674753, 0.25666791, 0.25664628, 0.2568103 , 0.25675501,\n",
       "        0.25667123, 0.25685978, 0.25668414, 0.25662083, 0.25660147,\n",
       "        0.25656987, 0.2568294 , 0.25657961, 0.2567869 , 0.25680261,\n",
       "        0.25653914, 0.25661159, 0.25661015, 0.2571044 , 0.25673018,\n",
       "        0.25659973, 0.25668526, 0.25667396, 0.25678465, 0.25689632,\n",
       "        0.25665918, 0.25663636, 0.25660827, 0.25656803, 0.25665084,\n",
       "        0.25657156, 0.25663867, 0.25645426, 0.25661448, 0.25660867,\n",
       "        0.25673473, 0.25667332, 0.2567943 , 0.25664415, 0.25668138,\n",
       "        0.25674625, 0.25668232, 0.25671992, 0.25673195, 0.25666245,\n",
       "        0.25671255, 0.25700126, 0.25674862, 0.25665961, 0.257     ,\n",
       "        0.25678243, 0.25665131, 0.25673617, 0.25664627, 0.25649876,\n",
       "        0.2566835 , 0.25662754, 0.25668105, 0.25653922, 0.25665509,\n",
       "        0.25661182, 0.25671186, 0.25672759, 0.25650015, 0.25682774,\n",
       "        0.25671955, 0.25665594, 0.25657711, 0.25680692, 0.25680577,\n",
       "        0.25667522, 0.25668448, 0.25665538, 0.25678262, 0.25651402,\n",
       "        0.25668539, 0.25658708, 0.25658656, 0.2566209 , 0.25662122,\n",
       "        0.25669069, 0.25658421, 0.25680925, 0.25651853, 0.25662928,\n",
       "        0.25669865, 0.25675003, 0.25651401, 0.25668776, 0.25658957,\n",
       "        0.25664142, 0.25711022, 0.25658317, 0.25684196, 0.25672476,\n",
       "        0.25657205, 0.25658629, 0.25653426, 0.25661357, 0.25704225,\n",
       "        0.25662987, 0.2566088 , 0.25651822, 0.25665429, 0.25646856,\n",
       "        0.25666419, 0.25668725, 0.25688754, 0.25669276, 0.25696349,\n",
       "        0.25657804, 0.25663893, 0.25670739, 0.25660534, 0.2566224 ,\n",
       "        0.25679244, 0.25662725, 0.25672334, 0.25653277, 0.25674802,\n",
       "        0.25681366, 0.25699725, 0.25648014, 0.25658171, 0.25680904,\n",
       "        0.25676389, 0.256692  , 0.25683624, 0.25659659, 0.25679031,\n",
       "        0.25667734, 0.2568136 , 0.25663005, 0.25709393, 0.25676354,\n",
       "        0.25664616, 0.25656129, 0.2567194 , 0.2566414 ]),\n",
       " 'Density': array([0.98595916, 0.98598649, 0.985928  , 0.98617263, 0.98591154,\n",
       "        0.98603579, 0.985657  , 0.98608192, 0.98591141, 0.98593753,\n",
       "        0.98592272, 0.98610341, 0.98544908, 0.98551858, 0.98607351,\n",
       "        0.9861282 , 0.98612588, 0.98602265, 0.98611624, 0.98592475,\n",
       "        0.98596993, 0.98599049, 0.98597634, 0.98560785, 0.98600935,\n",
       "        0.9860418 , 0.98553034, 0.98581848, 0.98593809, 0.98595977,\n",
       "        0.98613074, 0.9860187 , 0.98614973, 0.9861615 , 0.98597263,\n",
       "        0.98595125, 0.9859607 , 0.98602951, 0.98581996, 0.9854552 ,\n",
       "        0.98592551, 0.98598299, 0.98607462, 0.98596993, 0.98591337,\n",
       "        0.98585687, 0.98601671, 0.98614986, 0.98618021, 0.98565009,\n",
       "        0.9860231 , 0.98617035, 0.98600815, 0.98603499, 0.98608866,\n",
       "        0.98592438, 0.98612403, 0.98616202, 0.98569498, 0.985837  ,\n",
       "        0.9859313 , 0.98607143, 0.98589497, 0.98584703, 0.98586015,\n",
       "        0.98609725, 0.98605239, 0.98603309, 0.98595685, 0.98577155,\n",
       "        0.98604196, 0.9860938 , 0.98600783, 0.98616254, 0.98604355,\n",
       "        0.98560861, 0.98596639, 0.98578403, 0.98544087, 0.98616539,\n",
       "        0.9859618 , 0.98600189, 0.98578938, 0.98598039, 0.98577892,\n",
       "        0.98606884, 0.98608672, 0.98592004, 0.98602041, 0.98591561,\n",
       "        0.98591389, 0.98608573, 0.98585869, 0.98606449, 0.98602551,\n",
       "        0.98596453, 0.98609193, 0.98597588, 0.98595249, 0.98594673,\n",
       "        0.98608498, 0.98606528, 0.98608721, 0.98604622, 0.98606155,\n",
       "        0.98578725, 0.98548877, 0.98620348, 0.98596747, 0.9861345 ,\n",
       "        0.98581538, 0.98587162, 0.98573594, 0.98600951, 0.98606706,\n",
       "        0.98611019, 0.98593193, 0.98598191, 0.98590589, 0.98588306,\n",
       "        0.98616688, 0.9858159 , 0.98602541, 0.98615084, 0.98612755,\n",
       "        0.98619979, 0.98595304, 0.98582464, 0.98610622, 0.98615841,\n",
       "        0.98584112, 0.98585395, 0.98600008, 0.98555998, 0.98608215,\n",
       "        0.98570228, 0.98603795, 0.98611753, 0.9861421 , 0.98598355,\n",
       "        0.98604826, 0.98579486, 0.98608596, 0.98611384, 0.98601057,\n",
       "        0.98606693, 0.98594782, 0.98605389, 0.98615357, 0.98597728,\n",
       "        0.98573596, 0.98609075, 0.98594332, 0.98591291, 0.9858467 ,\n",
       "        0.98593158, 0.98603031, 0.98581898, 0.98611386, 0.98606567,\n",
       "        0.98594003, 0.9858552 , 0.98597037, 0.98591453, 0.98605935,\n",
       "        0.9856547 , 0.98591389, 0.98619752, 0.98592393, 0.98610048,\n",
       "        0.98605769, 0.98613541, 0.98599967, 0.9860835 , 0.98588228,\n",
       "        0.98591857, 0.98583036, 0.98608909, 0.98603174, 0.98581289,\n",
       "        0.98597952, 0.98603714, 0.98594361, 0.98604609, 0.98614282,\n",
       "        0.98597572, 0.98598989, 0.9860538 , 0.98572883, 0.98557668,\n",
       "        0.98598658, 0.98559406, 0.98601658, 0.98612443, 0.98603901,\n",
       "        0.98589265, 0.98588064, 0.98590775, 0.98605839, 0.98597506,\n",
       "        0.98559601, 0.98580644, 0.98567777, 0.98585439, 0.98587979,\n",
       "        0.98602297, 0.98620892, 0.98608633, 0.9860576 , 0.98610262,\n",
       "        0.98579725, 0.98602093, 0.98588654, 0.98614906, 0.98577647,\n",
       "        0.98542916, 0.9858786 , 0.98601459, 0.98606849, 0.98616778,\n",
       "        0.98593497, 0.98613491, 0.98617331, 0.9857939 , 0.98607613,\n",
       "        0.98580659, 0.9860324 , 0.98608767, 0.98599481, 0.98572549,\n",
       "        0.98593166, 0.98590088, 0.98592357, 0.98611806, 0.98571419,\n",
       "        0.98551573, 0.98606224, 0.98615376, 0.98579127, 0.98604016,\n",
       "        0.98598536, 0.98570371, 0.98594133, 0.98609807, 0.98570404,\n",
       "        0.98616612, 0.98604619, 0.9858655 , 0.98584795, 0.98604538,\n",
       "        0.98590396, 0.98608018, 0.98600908, 0.98606149, 0.98569603,\n",
       "        0.98599948, 0.98580187, 0.98618208, 0.98591613, 0.98591941,\n",
       "        0.98589018, 0.98598623, 0.98596546, 0.98592984, 0.98588491,\n",
       "        0.98605463, 0.98580145, 0.98606976, 0.98611713, 0.98611086,\n",
       "        0.98613749, 0.98580501, 0.98617452, 0.98587904, 0.98567626,\n",
       "        0.98620365, 0.98599381, 0.98592072, 0.98560814, 0.98605346,\n",
       "        0.98614185, 0.98606002, 0.9857246 , 0.98555208, 0.9857594 ,\n",
       "        0.98564171, 0.9860033 , 0.98610739, 0.98620243, 0.98610078,\n",
       "        0.98575163, 0.98603826, 0.98601748, 0.985857  , 0.98613654,\n",
       "        0.98598191, 0.98598889, 0.98593126, 0.98579357, 0.98598954,\n",
       "        0.98597403, 0.98602401, 0.9860056 , 0.98600509, 0.98614944,\n",
       "        0.98585065, 0.9855229 , 0.98601554, 0.98608019, 0.98552421,\n",
       "        0.98567032, 0.98586242, 0.98600692, 0.98592399, 0.98609266,\n",
       "        0.98611572, 0.98559908, 0.98600394, 0.98587527, 0.98601716,\n",
       "        0.98608599, 0.9859564 , 0.98590021, 0.98593664, 0.98590393,\n",
       "        0.98592629, 0.98595388, 0.98598289, 0.98566825, 0.98577608,\n",
       "        0.98601498, 0.98601999, 0.98606582, 0.98589904, 0.98605772,\n",
       "        0.986021  , 0.98612997, 0.98613998, 0.9859069 , 0.98606022,\n",
       "        0.98601146, 0.9859083 , 0.98560451, 0.98601656, 0.98614328,\n",
       "        0.9860628 , 0.98582031, 0.986145  , 0.9859211 , 0.9861333 ,\n",
       "        0.98612757, 0.98551719, 0.98590093, 0.98573373, 0.9857734 ,\n",
       "        0.98610464, 0.9861224 , 0.98613947, 0.98606044, 0.98548482,\n",
       "        0.98585686, 0.98589732, 0.98604569, 0.98568933, 0.98606027,\n",
       "        0.98579034, 0.98575369, 0.98551982, 0.98587013, 0.98567427,\n",
       "        0.98606516, 0.98610477, 0.98601375, 0.98615626, 0.98606971,\n",
       "        0.98593396, 0.98609775, 0.98597755, 0.98584308, 0.9859868 ,\n",
       "        0.98573855, 0.98558249, 0.98604585, 0.98612371, 0.98588296,\n",
       "        0.98586129, 0.98605016, 0.98571955, 0.98605462, 0.98568891,\n",
       "        0.98609407, 0.98571066, 0.98603688, 0.98550298, 0.98585463,\n",
       "        0.98612093, 0.98589036, 0.98579586, 0.98588789]),\n",
       " 'Rg': array([16.42043979, 16.42101675, 16.43532856, 16.42650443, 16.43468653,\n",
       "        16.43545522, 16.43246262, 16.42292204, 16.43206712, 16.4354569 ,\n",
       "        16.43699941, 16.43291763, 16.42563422, 16.42158558, 16.43348345,\n",
       "        16.43587512, 16.43584811, 16.43532356, 16.43382857, 16.43264712,\n",
       "        16.43410533, 16.432     , 16.43466457, 16.43206773, 16.43316345,\n",
       "        16.42816074, 16.435691  , 16.422703  , 16.42163466, 16.4275144 ,\n",
       "        16.43136002, 16.42873795, 16.43130732, 16.43276477, 16.41805646,\n",
       "        16.42651997, 16.43389178, 16.4357961 , 16.41318133, 16.44083326,\n",
       "        16.42655191, 16.43602245, 16.4299553 , 16.43184365, 16.43486611,\n",
       "        16.42295775, 16.4352091 , 16.43562177, 16.42408447, 16.4256974 ,\n",
       "        16.42262965, 16.43473952, 16.43204437, 16.42720371, 16.42417745,\n",
       "        16.43290705, 16.43513941, 16.43054785, 16.42638079, 16.42936365,\n",
       "        16.42373824, 16.42538351, 16.41120367, 16.42559602, 16.43194629,\n",
       "        16.43549335, 16.436159  , 16.43228756, 16.42544171, 16.41873728,\n",
       "        16.43580573, 16.43476031, 16.43127709, 16.4343044 , 16.43750953,\n",
       "        16.4333809 , 16.43454956, 16.41551983, 16.44074178, 16.43442881,\n",
       "        16.42741615, 16.41830741, 16.43191995, 16.43464697, 16.42403046,\n",
       "        16.43555706, 16.4309807 , 16.43098049, 16.4306889 , 16.43193708,\n",
       "        16.43161573, 16.42381349, 16.42872837, 16.43357097, 16.42786957,\n",
       "        16.42968584, 16.43173384, 16.43572098, 16.42848638, 16.43274762,\n",
       "        16.42268706, 16.4355089 , 16.43589871, 16.43251865, 16.43574946,\n",
       "        16.43339443, 16.4307025 , 16.42938946, 16.4198511 , 16.43471423,\n",
       "        16.42436584, 16.42592593, 16.43369595, 16.4368454 , 16.43247167,\n",
       "        16.43380798, 16.42803011, 16.42654588, 16.430269  , 16.42811767,\n",
       "        16.42810488, 16.41633847, 16.42429045, 16.43172934, 16.43075082,\n",
       "        16.43376193, 16.42674551, 16.43081302, 16.43402294, 16.43232982,\n",
       "        16.42905019, 16.42082755, 16.4232539 , 16.42683809, 16.43353122,\n",
       "        16.42758946, 16.43605889, 16.43433167, 16.43390048, 16.42374812,\n",
       "        16.43407235, 16.43344035, 16.43421045, 16.43309786, 16.41895446,\n",
       "        16.43532589, 16.42496401, 16.43488131, 16.43153912, 16.43665256,\n",
       "        16.42690872, 16.43775685, 16.43514836, 16.43451017, 16.43250513,\n",
       "        16.43525873, 16.43571134, 16.43144355, 16.43074104, 16.42712282,\n",
       "        16.43610511, 16.42981854, 16.42794387, 16.41833293, 16.43654735,\n",
       "        16.43373853, 16.41276533, 16.43119629, 16.42273391, 16.43620529,\n",
       "        16.42627053, 16.43300174, 16.43687296, 16.43648109, 16.42893732,\n",
       "        16.43009541, 16.43367571, 16.4354364 , 16.42934871, 16.43389251,\n",
       "        16.43540614, 16.42709193, 16.42390443, 16.41719666, 16.43526875,\n",
       "        16.43389822, 16.43034058, 16.43347454, 16.42898588, 16.43518136,\n",
       "        16.42850716, 16.42640511, 16.42746786, 16.43149937, 16.43687936,\n",
       "        16.42216661, 16.42630608, 16.43052051, 16.43244855, 16.43068515,\n",
       "        16.43630048, 16.42828706, 16.43305115, 16.41858317, 16.43035273,\n",
       "        16.4290322 , 16.43171524, 16.4314027 , 16.43601551, 16.43191214,\n",
       "        16.43430666, 16.41840818, 16.42375637, 16.42676617, 16.41983699,\n",
       "        16.44114877, 16.42037841, 16.4357935 , 16.43546514, 16.43198325,\n",
       "        16.43276853, 16.43416387, 16.43151044, 16.4329051 , 16.43469118,\n",
       "        16.4301097 , 16.43145403, 16.43525984, 16.43756518, 16.42271541,\n",
       "        16.43430409, 16.4310454 , 16.42372591, 16.43218823, 16.41672508,\n",
       "        16.43934648, 16.43044138, 16.43259028, 16.42036037, 16.43344464,\n",
       "        16.43010507, 16.43401603, 16.43202001, 16.43569842, 16.42421139,\n",
       "        16.43453393, 16.4375552 , 16.43198687, 16.43030785, 16.42426636,\n",
       "        16.4286295 , 16.43426606, 16.43219737, 16.43561298, 16.42054379,\n",
       "        16.42699123, 16.42087587, 16.43442662, 16.43407913, 16.43174234,\n",
       "        16.43449124, 16.42893122, 16.43206118, 16.43498305, 16.42977556,\n",
       "        16.43629475, 16.4339589 , 16.43520032, 16.43483248, 16.43408909,\n",
       "        16.43223647, 16.43076466, 16.43272464, 16.42968329, 16.42493159,\n",
       "        16.43147087, 16.42476549, 16.42894925, 16.43909005, 16.43798526,\n",
       "        16.43346293, 16.43614665, 16.42759155, 16.4233706 , 16.43353315,\n",
       "        16.42021757, 16.42752598, 16.43296664, 16.43342292, 16.43472444,\n",
       "        16.41530012, 16.43333624, 16.41927662, 16.42329155, 16.43148023,\n",
       "        16.43730878, 16.43349787, 16.43494863, 16.4219213 , 16.43054983,\n",
       "        16.43394492, 16.43261082, 16.43688394, 16.43768439, 16.43655487,\n",
       "        16.42659582, 16.4301474 , 16.43448766, 16.43575102, 16.43003303,\n",
       "        16.43109691, 16.42838363, 16.43530887, 16.42971766, 16.42952514,\n",
       "        16.43687911, 16.41880759, 16.43040882, 16.42245142, 16.43418154,\n",
       "        16.43371563, 16.43493784, 16.43486736, 16.42115098, 16.43428283,\n",
       "        16.43261916, 16.42558811, 16.4246783 , 16.42372643, 16.42808363,\n",
       "        16.43475521, 16.433426  , 16.43326154, 16.43625663, 16.42618016,\n",
       "        16.43171167, 16.43042146, 16.43124261, 16.42305519, 16.43231645,\n",
       "        16.4329762 , 16.42446223, 16.42840196, 16.42298946, 16.43373421,\n",
       "        16.43271801, 16.42585093, 16.43048888, 16.42616813, 16.43314215,\n",
       "        16.43505966, 16.43688932, 16.42043702, 16.42919018, 16.42838124,\n",
       "        16.4295251 , 16.43170268, 16.43073894, 16.430987  , 16.43378004,\n",
       "        16.42226655, 16.42351661, 16.423092  , 16.42415019, 16.41911272,\n",
       "        16.42163572, 16.42390781, 16.42626988, 16.43026921, 16.43469511,\n",
       "        16.43076361, 16.43443857, 16.43591882, 16.43325848, 16.43303804,\n",
       "        16.43356454, 16.43261546, 16.43068858, 16.4221986 , 16.43523622,\n",
       "        16.4313527 , 16.43298663, 16.42010522, 16.43342626, 16.43525696,\n",
       "        16.42960716, 16.43714245, 16.43092409, 16.42794496, 16.42576452,\n",
       "        16.43626962, 16.4281181 , 16.42868381, 16.43416257, 16.42916969,\n",
       "        16.43407627, 16.4225896 , 16.42775845, 16.42784902])}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = regressor.predict(list(df_train_subset2['SMILES']))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15c03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mse between all the values we have in df_train_subset2, vs what we predicted\n",
    "def MSE(results, df_train_subset2):\n",
    "    # now, for all the values we did have in df_train_subset2, we will see how different our predictions were from them\n",
    "    mask = df_train_subset2.iloc[:, -5:].isna().values\n",
    "    # convert results to array\n",
    "    results_arr = pd.DataFrame(results).values\n",
    "    diff = results_arr - df_train_subset2.iloc[:, -5:].values\n",
    "    mse = (diff[~mask]*diff[~mask]).mean()\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1751c804",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[192]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m regressor = NWRegressor(df_train_subset1, model, bw)\n\u001b[32m      4\u001b[39m regressor.embeddings = embeddings_df_train_subset1\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m results = \u001b[43mregressor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_train_subset2\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSMILES\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m mse = MSE(results, df_train_subset2)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMSE for bandwidth \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[183]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mNWRegressor.predict\u001b[39m\u001b[34m(self, smiles)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(smiles), \u001b[32m1\u001b[39m):\n\u001b[32m     63\u001b[39m     smiles_subset = smiles[i:i+\u001b[32m1000\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     test_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmiles_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# call the predict method for each property and return the values\u001b[39;00m\n\u001b[32m     66\u001b[39m     tg = np.concatenate([tg, \u001b[38;5;28mself\u001b[39m.predict_property(test_embeddings, \u001b[38;5;28mself\u001b[39m.data_tg)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:557\u001b[39m, in \u001b[36mencode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\n\u001b[32m    523\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    524\u001b[39m     sentences: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | np.ndarray,\n\u001b[32m   (...)\u001b[39m\u001b[32m    535\u001b[39m     **kwargs,\n\u001b[32m    536\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Tensor] | np.ndarray | Tensor | \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor] | \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]]:\n\u001b[32m    537\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    538\u001b[39m \u001b[33;03m    Computes sentence embeddings.\u001b[39;00m\n\u001b[32m    539\u001b[39m \n\u001b[32m    540\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m        sentences (Union[str, List[str]]): The sentences to embed.\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m        prompt_name (Optional[str], optional): The name of the prompt to use for encoding. Must be a key in the `prompts` dictionary,\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m            which is either set in the constructor or loaded from the model configuration. For example if\u001b[39;00m\n\u001b[32m    544\u001b[39m \u001b[33;03m            ``prompt_name`` is \"query\" and the ``prompts`` is {\"query\": \"query: \", ...}, then the sentence \"What\u001b[39;00m\n\u001b[32m    545\u001b[39m \u001b[33;03m            is the capital of France?\" will be encoded as \"query: What is the capital of France?\" because the sentence\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[33;03m            is appended to the prompt. If ``prompt`` is also set, this argument is ignored. Defaults to None.\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[33;03m        prompt (Optional[str], optional): The prompt to use for encoding. For example, if the prompt is \"query: \", then the\u001b[39;00m\n\u001b[32m    548\u001b[39m \u001b[33;03m            sentence \"What is the capital of France?\" will be encoded as \"query: What is the capital of France?\"\u001b[39;00m\n\u001b[32m    549\u001b[39m \u001b[33;03m            because the sentence is appended to the prompt. If ``prompt`` is set, ``prompt_name`` is ignored. Defaults to None.\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[33;03m        batch_size (int, optional): The batch size used for the computation. Defaults to 32.\u001b[39;00m\n\u001b[32m    551\u001b[39m \u001b[33;03m        show_progress_bar (bool, optional): Whether to output a progress bar when encode sentences. Defaults to None.\u001b[39;00m\n\u001b[32m    552\u001b[39m \u001b[33;03m        output_value (Optional[Literal[\"sentence_embedding\", \"token_embeddings\"]], optional): The type of embeddings to return:\u001b[39;00m\n\u001b[32m    553\u001b[39m \u001b[33;03m            \"sentence_embedding\" to get sentence embeddings, \"token_embeddings\" to get wordpiece token embeddings, and `None`,\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[33;03m            to get all output values. Defaults to \"sentence_embedding\".\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[33;03m        precision (Literal[\"float32\", \"int8\", \"uint8\", \"binary\", \"ubinary\"], optional): The precision to use for the embeddings.\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m            Can be \"float32\", \"int8\", \"uint8\", \"binary\", or \"ubinary\". All non-float32 precisions are quantized embeddings.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[33;03m            Quantized embeddings are smaller in size and faster to compute, but may have a lower accuracy. They are useful for\u001b[39;00m\n\u001b[32m    558\u001b[39m \u001b[33;03m            reducing the size of the embeddings of a corpus for semantic search, among other tasks. Defaults to \"float32\".\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[33;03m        convert_to_numpy (bool, optional): Whether the output should be a list of numpy vectors. If False, it is a list of PyTorch tensors.\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[33;03m            Defaults to True.\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[33;03m        convert_to_tensor (bool, optional): Whether the output should be one large tensor. Overwrites `convert_to_numpy`.\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[33;03m            Defaults to False.\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[33;03m        device (str, optional): Which :class:`torch.device` to use for the computation. Defaults to None.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[33;03m        normalize_embeddings (bool, optional): Whether to normalize returned vectors to have length 1. In that case,\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[33;03m            the faster dot-product (util.dot_score) instead of cosine similarity can be used. Defaults to False.\u001b[39;00m\n\u001b[32m    566\u001b[39m \n\u001b[32m    567\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[33;03m        Union[List[Tensor], ndarray, Tensor]: By default, a 2d numpy array with shape [num_inputs, output_dimension] is returned.\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[33;03m        If only one string input is provided, then the output is a 1d array with shape [output_dimension]. If ``convert_to_tensor``,\u001b[39;00m\n\u001b[32m    570\u001b[39m \u001b[33;03m        a torch Tensor is returned instead. If ``self.truncate_dim <= output_dimension`` then output_dimension is ``self.truncate_dim``.\u001b[39;00m\n\u001b[32m    571\u001b[39m \n\u001b[32m    572\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m        ::\u001b[39;00m\n\u001b[32m    574\u001b[39m \n\u001b[32m    575\u001b[39m \u001b[33;03m            from sentence_transformers import SentenceTransformer\u001b[39;00m\n\u001b[32m    576\u001b[39m \n\u001b[32m    577\u001b[39m \u001b[33;03m            # Load a pre-trained SentenceTransformer model\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[33;03m            model = SentenceTransformer('all-mpnet-base-v2')\u001b[39;00m\n\u001b[32m    579\u001b[39m \n\u001b[32m    580\u001b[39m \u001b[33;03m            # Encode some texts\u001b[39;00m\n\u001b[32m    581\u001b[39m \u001b[33;03m            sentences = [\u001b[39;00m\n\u001b[32m    582\u001b[39m \u001b[33;03m                \"The weather is lovely today.\",\u001b[39;00m\n\u001b[32m    583\u001b[39m \u001b[33;03m                \"It's so sunny outside!\",\u001b[39;00m\n\u001b[32m    584\u001b[39m \u001b[33;03m                \"He drove to the stadium.\",\u001b[39;00m\n\u001b[32m    585\u001b[39m \u001b[33;03m            ]\u001b[39;00m\n\u001b[32m    586\u001b[39m \u001b[33;03m            embeddings = model.encode(sentences)\u001b[39;00m\n\u001b[32m    587\u001b[39m \u001b[33;03m            print(embeddings.shape)\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[33;03m            # (3, 768)\u001b[39;00m\n\u001b[32m    589\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_hpu_graph_enabled:\n\u001b[32m    591\u001b[39m         \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhabana_frameworks\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mht\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:619\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    618\u001b[39m         prompt = \u001b[38;5;28mself\u001b[39m.prompts[prompt_name]\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    620\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    621\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrompt name \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not found in the configured prompts dictionary with keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.prompts.keys())\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    622\u001b[39m         )\n\u001b[32m    623\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_prompt_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.auto_model(**trans_features, **kwargs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    443\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    444\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/transformers/models/roberta/modeling_roberta.py:942\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    935\u001b[39m         extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n\u001b[32m    936\u001b[39m             attention_mask,\n\u001b[32m    937\u001b[39m             input_shape,\n\u001b[32m    938\u001b[39m             embedding_output,\n\u001b[32m    939\u001b[39m             past_key_values_length,\n\u001b[32m    940\u001b[39m         )\n\u001b[32m    941\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m         extended_attention_mask = \u001b[43m_prepare_4d_attention_mask_for_sdpa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseq_length\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    946\u001b[39m     \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[32m    947\u001b[39m     \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n\u001b[32m    948\u001b[39m     extended_attention_mask = \u001b[38;5;28mself\u001b[39m.get_extended_attention_mask(attention_mask, input_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:451\u001b[39m, in \u001b[36m_prepare_4d_attention_mask_for_sdpa\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAttentionMaskConverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_expand_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/transformers/modeling_attn_mask_utils.py:192\u001b[39m, in \u001b[36mAttentionMaskConverter._expand_mask\u001b[39m\u001b[34m(mask, dtype, tgt_len)\u001b[39m\n\u001b[32m    188\u001b[39m expanded_mask = mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :].expand(bsz, \u001b[32m1\u001b[39m, tgt_len, src_len).to(dtype)\n\u001b[32m    190\u001b[39m inverted_mask = \u001b[32m1.0\u001b[39m - expanded_mask\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minverted_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverted_mask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# loop through a range of bandwidth values and get the mse for each of them\n",
    "for bw in [1, 3, 5, 7, 10, 15, 20]:\n",
    "    regressor = NWRegressor(df_train_subset1, model, bw)\n",
    "    regressor.embeddings = embeddings_df_train_subset1\n",
    "    results = regressor.predict(list(df_train_subset2['SMILES']))\n",
    "    mse = MSE(results, df_train_subset2)\n",
    "    print(f'MSE for bandwidth {bw} is: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cde2b058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for bandwidth 0.1 is: 295.4577030396035\n",
      "MSE for bandwidth 0.5 is: 381.51585066733224\n",
      "MSE for bandwidth 1 is: 537.8446041863151\n",
      "MSE for bandwidth 3 is: 596.609191776331\n",
      "MSE for bandwidth 5 is: 601.4957767584965\n",
      "MSE for bandwidth 7 is: 602.8462505183746\n",
      "MSE for bandwidth 10 is: 603.5644226114141\n",
      "MSE for bandwidth 15 is: 603.9479373244706\n",
      "MSE for bandwidth 20 is: 604.0822210377564\n"
     ]
    }
   ],
   "source": [
    "# loop through a range of bandwidth values and get the mse for each of them\n",
    "for bw in [0.1, 0.5, 1, 3, 5, 7, 10, 15, 20]:\n",
    "    regressor = NWRegressor(df_train_subset1, model, bw)\n",
    "    regressor.embeddings = embeddings_df_train_subset1\n",
    "    results = regressor.predict(list(df_train_subset2['SMILES']))\n",
    "    mse = MSE(results, df_train_subset2)\n",
    "    print(f'MSE for bandwidth {bw} is: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "debbee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for bandwidth 0.09 is: 310.3871255423069\n"
     ]
    }
   ],
   "source": [
    "# loop through a range of bandwidth values and get the mse for each of them\n",
    "for bw in [0.09]:\n",
    "    regressor = NWRegressor(df_train_subset1, model, bw)\n",
    "    regressor.embeddings = embeddings_df_train_subset1\n",
    "    results = regressor.predict(list(df_train_subset2['SMILES']))\n",
    "    mse = MSE(results, df_train_subset2)\n",
    "    print(f'MSE for bandwidth {bw} is: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ae3c9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = NWRegressor(df_train, model, 10)\n",
    "regressor.embeddings = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fc9bae29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tg': array([96.5414254 , 96.53859863, 96.52507728]),\n",
       " 'FFV': array([0.36722244, 0.36721512, 0.36721965]),\n",
       " 'Tc': array([0.25633297, 0.25630172, 0.2563517 ]),\n",
       " 'Density': array([0.98554821, 0.98557284, 0.98552317]),\n",
       " 'Rg': array([16.42099139, 16.41996536, 16.42111135])}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = regressor.predict(list(df_test['SMILES']))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4bbbc3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tg': array([105.09817477, 104.73706845, 103.6838063 ]),\n",
       " 'FFV': array([0.36823022, 0.36748777, 0.36795999]),\n",
       " 'Tc': array([0.25606466, 0.25295064, 0.25783874]),\n",
       " 'Density': array([0.99242539, 0.99511359, 0.98956572]),\n",
       " 'Rg': array([16.56232519, 16.46827539, 16.55897016])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.bandwidth = 1\n",
    "results2 = regressor.predict(df_test['SMILES'])\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b176f5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tg': array([117.60633137, 126.80818897,  48.24818847]),\n",
       " 'FFV': array([0.38278817, 0.37967689, 0.35148612]),\n",
       " 'Tc': array([0.21379043, 0.22237086, 0.18213848]),\n",
       " 'Density': array([1.12927828, 1.10327563, 1.21231996]),\n",
       " 'Rg': array([21.22370989, 21.5836155 , 18.07928376])}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.bandwidth = 0.1\n",
    "results3 = regressor.predict(df_test['SMILES'])\n",
    "results3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "49904cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export into csv\n",
    "def SaveOutput(results_dict, output_filename, df_test):\n",
    "    df = pd.DataFrame()\n",
    "    # get id col from df_test\n",
    "    df['id'] = df_test['id']\n",
    "    # get other info from results_dict\n",
    "    df['Tg'] = results_dict['Tg']\n",
    "    df['FFV'] = results_dict['FFV']\n",
    "    df['Tc'] = results_dict['Tc']\n",
    "    df['Density'] = results_dict['Density']\n",
    "    df['Rg'] = results_dict['Rg']\n",
    "    # save to specified location\n",
    "    df.to_csv(output_filename, index = False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a7d17d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveOutput(results, 'nw_bw10.csv', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d930cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveOutput(results2, 'nw_bw1.csv', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "47e8a691",
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveOutput(results3, 'nw_bw1tenth.csv', df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bc46a87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>10142210</td>\n",
       "      <td>*NC(C)C(=O)NCC(=O)NCC(*)=O</td>\n",
       "      <td>208.639749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>13838538</td>\n",
       "      <td>*CCCCCCSSCCCCSS*</td>\n",
       "      <td>-41.266724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>16498242</td>\n",
       "      <td>*C=CCCCCCCCC*</td>\n",
       "      <td>-17.282022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>30582999</td>\n",
       "      <td>*CCCCCCCCCCOC(=O)c1ccc(C(=O)NCCNC(=O)c2ccc(C(=...</td>\n",
       "      <td>4.250403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>36217683</td>\n",
       "      <td>*c1nc2cc3sc(-c4cc(OCCCCCC)c(*)cc4OCCCCCC)nc3cc2s1</td>\n",
       "      <td>168.526313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7863</th>\n",
       "      <td>2116365788</td>\n",
       "      <td>*Nc1cc(SCCC#N)c(NC(=O)c2cccc(C(*)=O)c2)cc1SCCC#N</td>\n",
       "      <td>38.160660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7868</th>\n",
       "      <td>2117950580</td>\n",
       "      <td>*c1ccc(C2C(C(=O)OCC)C(*)C2C(=O)OCC)cc1</td>\n",
       "      <td>164.322463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7889</th>\n",
       "      <td>2124040823</td>\n",
       "      <td>*Oc1ccc(C=Cc2ccc(C=Cc3ccc(OC(=O)CCCCCCCCC(*)=O...</td>\n",
       "      <td>35.475235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7911</th>\n",
       "      <td>2130807414</td>\n",
       "      <td>*CC(*)C(=O)OCC1(C)COC(C)(C)OC1</td>\n",
       "      <td>95.741049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>2147435020</td>\n",
       "      <td>*C=C(*)c1ccccc1C</td>\n",
       "      <td>261.662355</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>511 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                             SMILES  \\\n",
       "40      10142210                         *NC(C)C(=O)NCC(=O)NCC(*)=O   \n",
       "57      13838538                                   *CCCCCCSSCCCCSS*   \n",
       "63      16498242                                      *C=CCCCCCCCC*   \n",
       "108     30582999  *CCCCCCCCCCOC(=O)c1ccc(C(=O)NCCNC(=O)c2ccc(C(=...   \n",
       "123     36217683  *c1nc2cc3sc(-c4cc(OCCCCCC)c(*)cc4OCCCCCC)nc3cc2s1   \n",
       "...          ...                                                ...   \n",
       "7863  2116365788   *Nc1cc(SCCC#N)c(NC(=O)c2cccc(C(*)=O)c2)cc1SCCC#N   \n",
       "7868  2117950580             *c1ccc(C2C(C(=O)OCC)C(*)C2C(=O)OCC)cc1   \n",
       "7889  2124040823  *Oc1ccc(C=Cc2ccc(C=Cc3ccc(OC(=O)CCCCCCCCC(*)=O...   \n",
       "7911  2130807414                     *CC(*)C(=O)OCC1(C)COC(C)(C)OC1   \n",
       "7971  2147435020                                   *C=C(*)c1ccccc1C   \n",
       "\n",
       "              Tg  \n",
       "40    208.639749  \n",
       "57    -41.266724  \n",
       "63    -17.282022  \n",
       "108     4.250403  \n",
       "123   168.526313  \n",
       "...          ...  \n",
       "7863   38.160660  \n",
       "7868  164.322463  \n",
       "7889   35.475235  \n",
       "7911   95.741049  \n",
       "7971  261.662355  \n",
       "\n",
       "[511 rows x 3 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a83b7f8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>SMILES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1109053969</td>\n",
       "      <td>*Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1422188626</td>\n",
       "      <td>*Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2032016830</td>\n",
       "      <td>*c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             SMILES\n",
       "0  1109053969  *Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...\n",
       "1  1422188626  *Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...\n",
       "2  2032016830  *c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6..."
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019fc7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "40fcf9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = SentenceTransformer('/Users/shashankkatiyar/.cache/huggingface/hub/models--Derify--ChemMRL-alpha/snapshots/10bace4387f5ce86181c382b110ece9cd55d9dcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a0b3b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b257e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e017dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "276413ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)(F)F)C(F)(F)F)cc3)cc2)cc1',\n",
       " '*Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c5ccc(*)cc5)c4)cc3)cc2)cc1',\n",
       " '*c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6c5C(=O)N(*)C6=O)cc4C3=O)c2)c1']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_test['SMILES'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e3cb6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = np.random.randn(10,1024)\n",
    "temp2 = np.random.randn(20,1024)\n",
    "similarities = model.similarity(temp1, temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f4cee2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 20])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "726ad490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a6b96e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.63327622, 32.68170798, 32.44342167, 31.57763404, 33.89966671,\n",
       "       31.79731525, 31.81958678, 32.31310153, 32.28397145, 31.48179124])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(temp1, ord = 2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4adfc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6355a4a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01229778, -1.22884953,  0.53899921, ...,  1.2770107 ,\n",
       "        -0.77500096, -0.23875563],\n",
       "       [ 0.98284899, -0.93888819, -0.44800889, ...,  0.04444339,\n",
       "         0.2609156 ,  0.66138676],\n",
       "       [-0.07738374, -0.09886296,  1.53370645, ...,  0.76334611,\n",
       "        -1.50325518, -1.71726304],\n",
       "       ...,\n",
       "       [ 0.41544835, -0.44895731,  0.35903934, ...,  0.35689733,\n",
       "        -1.41245859, -1.79132368],\n",
       "       [ 2.70032027, -0.19512066, -2.31937895, ...,  0.46694383,\n",
       "        -0.72352537, -0.44254963],\n",
       "       [ 1.15613964,  0.10046639, -0.9822714 , ...,  0.66516305,\n",
       "        -0.18370319,  2.55267478]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd3a6656",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (7973x1024 and 512x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m similarities = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/gsoc/lib/python3.12/site-packages/sentence_transformers/util.py:108\u001b[39m, in \u001b[36mcos_sim\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m    106\u001b[39m a_norm = normalize_embeddings(a)\n\u001b[32m    107\u001b[39m b_norm = normalize_embeddings(b)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_norm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (7973x1024 and 512x10)"
     ]
    }
   ],
   "source": [
    "similarities = model.similarity(temp2, temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dc1c00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6260, 0.6811],\n",
       "        [0.6260, 1.0000, 0.6901],\n",
       "        [0.6811, 0.6901, 1.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
